{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from multiprocessing import Process, Pool\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load reads from .fastq files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kmers(sequence, k):\n",
    "    \"\"\"\n",
    "    Generate k-mers from a sequence.\n",
    "    \"\"\"\n",
    "    return [sequence[i:i + k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "def generate_kmer_to_index_dict(k):\n",
    "    all_kmers = sorted([''.join(x) for x in itertools.product('ATGCN', repeat=k)])\n",
    "    return {k: v for v, k in enumerate(all_kmers)}\n",
    "    \n",
    "def kmer_to_index(kmer, kmer_to_index_dict):\n",
    "    return kmer_to_index_dict[kmer]\n",
    "\n",
    "def subkmer_frequencies_in_kmer(kmer, subkmer_length, kmer_to_index_dict):\n",
    "    \"\"\"Calculates the frequency of each subkmer in a kmer.\"\"\"\n",
    "    subkmer_counts = Counter(kmer[i:i+subkmer_length] for i in range(len(kmer) - subkmer_length + 1))\n",
    "    frequencies = np.zeros(5**subkmer_length)\n",
    "    for subkmer, count in subkmer_counts.items():\n",
    "        index = kmer_to_index(subkmer, kmer_to_index_dict)\n",
    "        frequencies[index] = count\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AA': 0,\n",
       " 'AC': 1,\n",
       " 'AG': 2,\n",
       " 'AN': 3,\n",
       " 'AT': 4,\n",
       " 'CA': 5,\n",
       " 'CC': 6,\n",
       " 'CG': 7,\n",
       " 'CN': 8,\n",
       " 'CT': 9,\n",
       " 'GA': 10,\n",
       " 'GC': 11,\n",
       " 'GG': 12,\n",
       " 'GN': 13,\n",
       " 'GT': 14,\n",
       " 'NA': 15,\n",
       " 'NC': 16,\n",
       " 'NG': 17,\n",
       " 'NN': 18,\n",
       " 'NT': 19,\n",
       " 'TA': 20,\n",
       " 'TC': 21,\n",
       " 'TG': 22,\n",
       " 'TN': 23,\n",
       " 'TT': 24}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2mer_dict = generate_kmer_to_index_dict(2)\n",
    "sub2mer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_labels(data_path: str, outdir: str = None, save_to_json=True,\n",
    "                       data_exts: tuple[str] = ('.gz', '.fastq', '.fasta', '.fq', '.fa')) -> tuple[dict, dict]:\n",
    "    unique_codes = set()\n",
    "\n",
    "    for sample in os.listdir(data_path):\n",
    "        sample_filename = os.path.basename(sample)\n",
    "        sample_ext = os.path.splitext(sample_filename)[1]\n",
    "        if sample_ext not in data_exts:\n",
    "            continue\n",
    "\n",
    "        # Example: CAMDA20_MetaSUB_CSD16_BCN_012_1_kneaddata_subsampled_20_percent.fastq\n",
    "        city_id = list(sample_filename.split('_'))[3]\n",
    "\n",
    "        unique_codes.add(city_id)\n",
    "\n",
    "    unique_codes_list = list(unique_codes)\n",
    "    id_to_code = {}\n",
    "    code_to_id = {}\n",
    "    for idx, code in enumerate(unique_codes_list):\n",
    "        id_to_code[idx] = code\n",
    "        code_to_id[code] = idx\n",
    "\n",
    "    if outdir:\n",
    "        id_to_code_outfile = os.path.join(outdir, 'id_to_code.json')\n",
    "        code_to_id_outfile = os.path.join(outdir, 'code_to_id.json')\n",
    "    else:\n",
    "        id_to_code_outfile = 'id_to_code.json'\n",
    "        code_to_id_outfile = 'code_to_id.json'\n",
    "\n",
    "    if save_to_json:\n",
    "        with open(id_to_code_outfile, 'w') as f:\n",
    "            json.dump(id_to_code, f)\n",
    "\n",
    "        with open(code_to_id_outfile, 'w') as f:\n",
    "            json.dump(code_to_id, f)\n",
    "\n",
    "    return id_to_code, code_to_id\n",
    "\n",
    "\n",
    "def get_reads_from_fq(fq_path: str) -> list[str]:\n",
    "    reads = []\n",
    "    with open(fq_path, 'r') as f:\n",
    "        fastq_reads = f.readlines()\n",
    "        for i in range(0, len(fastq_reads), 4):\n",
    "            reads.append(str(fastq_reads[i + 1].rstrip()))\n",
    "    return reads\n",
    "\n",
    "\n",
    "def get_labeled_reads_from_dir_with_samples(indir: str) -> dict:\n",
    "    reads_for_samples = {} # dict\n",
    "    id_to_code, code_to_id = parse_train_labels(data_path=indir, save_to_json=False)\n",
    "    files_in_dir = os.listdir(indir)\n",
    "    for file in files_in_dir:\n",
    "        print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} processing file {file}')\n",
    "        if os.path.splitext(file)[1] != '.fastq':\n",
    "            print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} skipping {file}')\n",
    "            continue\n",
    "        city_code = os.path.basename(file).split('_')[3]\n",
    "        sample_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        int_label = int(code_to_id[city_code])\n",
    "        print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} {city_code = } ; {int_label = }')\n",
    "        print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} getting reads')\n",
    "        reads = get_reads_from_fq(os.path.join(indir, file))\n",
    "        print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} saving labelled reads')\n",
    "        if sample_name not in reads_for_samples:\n",
    "            reads_for_samples[sample_name] = [int_label, reads]\n",
    "        else:\n",
    "            reads_for_samples[sample_name][1].extend(reads)\n",
    "    return reads_for_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07 Aug 2024 21:27:44 processing file CAMDA20_MetaSUB_CSD17_HKG_010_1_kneaddata_subsampled_20_percent.fastq\n",
      "07 Aug 2024 21:27:44 city_code = 'HKG' ; int_label = 0\n",
      "07 Aug 2024 21:27:44 getting reads\n",
      "07 Aug 2024 21:27:44 saving labelled reads\n",
      "07 Aug 2024 21:27:44 processing file dbgs\n",
      "07 Aug 2024 21:27:44 skipping dbgs\n"
     ]
    }
   ],
   "source": [
    "genome_sequences = get_labeled_reads_from_dir_with_samples('/path/to/dir/with/fastq/samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/path/to/graphs/outdir'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "kmer_len = 4\n",
    "subkmer_len = 2\n",
    "num_features = 5**subkmer_len\n",
    "sub2mer_to_dict = generate_kmer_to_index_dict(subkmer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_max(dict_item):\n",
    "    sample_name, code_and_reads = dict_item\n",
    "    city_code, seqs = code_and_reads\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    kmers = set()\n",
    "    print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} getting k-mers from {len(seqs)} reads')\n",
    "    transition_counts = defaultdict(int)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        if idx % 100_000 == 0:\n",
    "            print(f'processed {idx} reads')\n",
    "        kmers_in_read = generate_kmers(seq, kmer_len)\n",
    "        kmers = kmers.union(set(kmers_in_read))\n",
    "        for kk in range(len(kmers_in_read) - 1):\n",
    "            transition_counts[(kmers_in_read[kk], kmers_in_read[kk + 1])] += 1\n",
    "    nodes = []\n",
    "    print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} adding nodes to graph')\n",
    "    for kmer in kmers:\n",
    "        nodes.append((kmer, {\"x\": torch.as_tensor(subkmer_frequencies_in_kmer(kmer, subkmer_len, sub2mer_to_dict)/(kmer_len-1), dtype=torch.float32)}))\n",
    "    G.add_nodes_from(nodes)\n",
    "\n",
    "    max_count = max(transition_counts.values())\n",
    "    print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} adding edges')\n",
    "    for key in transition_counts.keys():\n",
    "        G.add_edge(key[0], key[1], weight=transition_counts[key]/max_count)\n",
    "\n",
    "    print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} saving as torch graph')\n",
    "    torch_graph = from_networkx(G)\n",
    "    torch_graph['y'] = torch.tensor([city_code])\n",
    "\n",
    "    print(f'{datetime.datetime.now().strftime(\"%d %h %Y %H:%M:%S\")} saving graph for sample {sample_name}')\n",
    "    outfile_graph_name = os.path.join(outdir, sample_name + '.labeled_graph_max')\n",
    "    with open(outfile_graph_name, 'wb') as f:\n",
    "        pickle.dump(torch_graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07 Aug 2024 21:32:14 getting k-mers from 535034 reads\n",
      "processed 0 reads\n",
      "processed 100000 reads\n",
      "processed 200000 reads\n",
      "processed 300000 reads\n",
      "processed 400000 reads\n",
      "processed 500000 reads\n",
      "07 Aug 2024 21:32:32 adding nodes to graph\n",
      "07 Aug 2024 21:32:32 adding edges\n",
      "07 Aug 2024 21:32:32 saving as torch graph\n",
      "07 Aug 2024 21:32:32 saving graph for sample CAMDA20_MetaSUB_CSD17_HKG_010_1_kneaddata_subsampled_20_percent\n"
     ]
    }
   ],
   "source": [
    "for genome_seq in genome_sequences.items():\n",
    "    build_graph_max(genome_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNs",
   "language": "python",
   "name": "gnns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
